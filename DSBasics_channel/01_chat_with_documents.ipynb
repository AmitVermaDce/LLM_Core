{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Token Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging in with available token\n"
     ]
    }
   ],
   "source": [
    "# Step 0:\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the Hugging Face token from the environment variables\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "# Use the token for login or other operations\n",
    "def login(token):\n",
    "    if token:\n",
    "        print(f\"Logging in with available token\")\n",
    "    else:\n",
    "        print(\"No token provided.\")\n",
    "\n",
    "login(token=HF_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize LLM - HuggingFace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully initialized LLM with model: mistralai/Mixtral-8x7B-Instruct-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RFMD1350\\AppData\\Local\\Temp\\ipykernel_23016\\3505731649.py:11: DeprecationWarning: Call to deprecated class HuggingFaceInferenceAPI. (Deprecated in favor of `HuggingFaceInferenceAPI` from `llama-index-llms-huggingface-api` which should be used instead.)\n",
      "  llm = HuggingFaceInferenceAPI(\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize LLM Model\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceInferenceAPI\n",
    "\n",
    "def initialize_llm(model_name, token):\n",
    "    \"\"\"Initialize the Hugging Face Inference API with the specified model and token.\"\"\"\n",
    "    if not token:\n",
    "        raise ValueError(\"API token must be provided.\")\n",
    "    \n",
    "    try:\n",
    "        llm = HuggingFaceInferenceAPI(\n",
    "            model_name=model_name,\n",
    "            token=token\n",
    "        )\n",
    "        print(f\"Successfully initialized LLM with model: {model_name}\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to initialize LLM: {e}\")\n",
    "\n",
    "# Specify the model name\n",
    "MODEL_NAME = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "\n",
    "# Initialize the LLM\n",
    "try:\n",
    "    llm_instance = initialize_llm(MODEL_NAME, HF_TOKEN) # It will be needed in subsequent cells\n",
    "    # print(llm_instance)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Embedding Model - HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model 'model_name='BAAI/bge-small-en-v1.5' embed_batch_size=10 callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x000001EB50871510> num_workers=None max_length=512 normalize=True query_instruction=None text_instruction=None cache_folder=None' initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Initialize Embedding model if required\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "import time\n",
    "\n",
    "\n",
    "def initialize_embedding_model(embed_model_name):\n",
    "    \"\"\"Initialize the HuggingFaceEmbedding model.\"\"\"\n",
    "    try:\n",
    "        embed_model = HuggingFaceEmbedding(model_name=embed_model_name)\n",
    "        Settings.embed_model = embed_model  # Set it globally for use in LlamaIndex\n",
    "        print(f\"Embedding model '{embed_model}' initialized successfully.\")\n",
    "        return embed_model\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to initialize embedding model: {e}\")\n",
    "    \n",
    "\n",
    "# Specify the EMBEDDING model name\n",
    "EMBED_MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
    "\n",
    "# Initialize the LLM\n",
    "try:\n",
    "    embedding_model = initialize_embedding_model(EMBED_MODEL_NAME) # It will be needed in subsequent cells\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and renaming PDF file from a path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF downloaded and saved as attention-is-all-you-need.pdf in ../artifacts.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Downloading and Renaming a PDF file\n",
    "import requests\n",
    "\n",
    "def download_pdf(pdf_url, download_dir, new_filename):\n",
    "    \"\"\"Downloads a PDF file from the specified URL and saves it to the given directory with the new filename.\"\"\"\n",
    "    # Create the directory if it doesn't exist\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "    # Downloading using requests\n",
    "    response = requests.get(pdf_url)\n",
    "    if response.status_code == 200:\n",
    "        with open(os.path.join(download_dir, new_filename), 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"PDF downloaded and saved as {new_filename} in {download_dir}.\")\n",
    "    else:\n",
    "        print(f\"Failed to download PDF: {response.status_code}\")\n",
    "\n",
    "# Specify the URL of the PDF file you want to download\n",
    "pdf_url = \"https://arxiv.org/pdf/1706.03762\"\n",
    "\n",
    "# Specify the directory where you want to save the PDF file\n",
    "download_dir = \"../artifacts\"\n",
    "\n",
    "# Specify the desired new name for the downloaded PDF file\n",
    "new_filename = \"attention-is-all-you-need.pdf\"\n",
    "\n",
    "# Call the function to download the PDF within a try-except block\n",
    "try:\n",
    "    download_pdf(pdf_url, download_dir, new_filename)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded 15 document(s).\n",
      "\n",
      "\n",
      "Doc ID: e05229d7-f2b8-4499-bcc0-548ed4f4a623\n",
      "Text: Provided proper attribution is provided, Google hereby grants\n",
      "permission to reproduce the tables and figures in this paper solely\n",
      "for use in journalistic or scholarly works. Attention Is All You Need\n",
      "Ashish Vaswani∗ Google Brain avaswani@google.com Noam Shazeer∗ Google\n",
      "Brain noam@google.com Niki Parmar∗ Google Research nikip@google.com\n",
      "Jakob Usz...\n",
      "\n",
      "\n",
      "Doc ID: e47bcfac-4893-413f-8bdb-db50bd45b253\n",
      "Text: 1 Introduction Recurrent neural networks, long short-term memory\n",
      "[13] and gated recurrent [7] neural networks in particular, have been\n",
      "firmly established as state of the art approaches in sequence modeling\n",
      "and transduction problems such as language modeling and machine\n",
      "translation [ 35, 2, 5]. Numerous efforts have since continued to push\n",
      "the bo...\n",
      "\n",
      "\n",
      "Doc ID: 301c344d-897e-47dd-b773-a1ebab2f2667\n",
      "Text: Figure 1: The Transformer - model architecture. The Transformer\n",
      "follows this overall architecture using stacked self-attention and\n",
      "point-wise, fully connected layers for both the encoder and decoder,\n",
      "shown in the left and right halves of Figure 1, respectively. 3.1\n",
      "Encoder and Decoder Stacks Encoder: The encoder is composed of a stack\n",
      "of N = 6 i...\n",
      "\n",
      "\n",
      "Doc ID: abc9596a-7c18-4406-8aec-9831d7cf42c3\n",
      "Text: Scaled Dot-Product Attention  Multi-Head Attention Figure 2:\n",
      "(left) Scaled Dot-Product Attention. (right) Multi-Head Attention\n",
      "consists of several attention layers running in parallel. of the\n",
      "values, where the weight assigned to each value is computed by a\n",
      "compatibility function of the query with the corresponding key. 3.2.1\n",
      "Scaled Dot-Product A...\n",
      "\n",
      "\n",
      "Doc ID: a2e047e1-f747-416c-ae7b-2483ba9e324a\n",
      "Text: output values. These are concatenated and once again projected,\n",
      "resulting in the final values, as depicted in Figure 2. Multi-head\n",
      "attention allows the model to jointly attend to information from\n",
      "different representation subspaces at different positions. With a\n",
      "single attention head, averaging inhibits this. MultiHead(Q, K, V) =\n",
      "Concat(head1, .....\n",
      "\n",
      "\n",
      "Doc ID: a6c9e6a8-1ad9-4858-ae30-81349c129711\n",
      "Text: Table 1: Maximum path lengths, per-layer complexity and minimum\n",
      "number of sequential operations for different layer types. n is the\n",
      "sequence length, d is the representation dimension, k is the kernel\n",
      "size of convolutions and r the size of the neighborhood in restricted\n",
      "self-attention. Layer Type Complexity per Layer Sequential Maximum\n",
      "Path Lengt...\n",
      "\n",
      "\n",
      "Doc ID: d8689d12-29d6-4d37-9de6-f3341383b72c\n",
      "Text: length n is smaller than the representation dimensionality d,\n",
      "which is most often the case with sentence representations used by\n",
      "state-of-the-art models in machine translations, such as word-piece\n",
      "[38] and byte-pair [31] representations. To improve computational\n",
      "performance for tasks involving very long sequences, self-attention\n",
      "could be restric...\n",
      "\n",
      "\n",
      "Doc ID: e25b68f9-f1e9-41fc-ac8c-1d3fe77b6c97\n",
      "Text: Table 2: The Transformer achieves better BLEU scores than\n",
      "previous state-of-the-art models on the English-to-German and English-\n",
      "to-French newstest2014 tests at a fraction of the training cost. Model\n",
      "BLEU Training Cost (FLOPs) EN-DE EN-FR EN-DE EN-FR ByteNet [18] 23.75\n",
      "Deep-Att + PosUnk [39] 39.2 1.0 · 1020 GNMT + RL [38] 24.6 39.92 2.3 ·\n",
      "1019 1....\n",
      "\n",
      "\n",
      "Doc ID: 28c5f57c-bb53-4bb2-ac47-14a17821313d\n",
      "Text: Table 3: Variations on the Transformer architecture. Unlisted\n",
      "values are identical to those of the base model. All metrics are on\n",
      "the English-to-German translation development set, newstest2013.\n",
      "Listed perplexities are per-wordpiece, according to our byte-pair\n",
      "encoding, and should not be compared to per-word perplexities. N d\n",
      "model dff h d k dv ...\n",
      "\n",
      "\n",
      "Doc ID: f9f77842-a9dc-4f4f-aab7-7171ea6cc652\n",
      "Text: Table 4: The Transformer generalizes well to English\n",
      "constituency parsing (Results are on Section 23 of WSJ) Parser\n",
      "Training WSJ 23 F1 Vinyals & Kaiser el al. (2014) [37] WSJ only,\n",
      "discriminative 88.3 Petrov et al. (2006) [29] WSJ only, discriminative\n",
      "90.4 Zhu et al. (2013) [40] WSJ only, discriminative 90.4 Dyer et al.\n",
      "(2016) [8] WSJ only, disc...\n",
      "\n",
      "\n",
      "Doc ID: cc41b94a-cdb0-450f-b9b5-c42f92d02ce1\n",
      "Text: [5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi\n",
      "Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase\n",
      "representations using rnn encoder-decoder for statistical machine\n",
      "translation. CoRR, abs/1406.1078, 2014. [6] Francois Chollet.\n",
      "Xception: Deep learning with depthwise separable convolutions. arXiv\n",
      "preprint arXiv:1610.02357...\n",
      "\n",
      "\n",
      "Doc ID: 31a4bc35-7e9a-4f20-a221-6ef145a37890\n",
      "Text: [25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice\n",
      "Santorini. Building a large annotated corpus of english: The penn\n",
      "treebank. Computational linguistics, 19(2):313–330, 1993. [26] David\n",
      "McClosky, Eugene Charniak, and Mark Johnson. Effective self-training\n",
      "for parsing. In Proceedings of the Human Language Technology\n",
      "Conference of the NAA...\n",
      "\n",
      "\n",
      "Doc ID: 5b548f72-5b15-43b4-a014-369afb1f10e8\n",
      "Text: Attention Visualizations Input-Input Layer5 It is in this spirit\n",
      "that a majority of American governments have passed new laws since\n",
      "2009 making the registration or voting process more difficult . <EOS>\n",
      "<pad> <pad> <pad> <pad> <pad> <pad> It is in this spirit that a\n",
      "majority of American governments have passed new laws since 2009\n",
      "making the regis...\n",
      "\n",
      "\n",
      "Doc ID: cdcb317f-7e57-4c7d-bfa7-ef6ccbefda4d\n",
      "Text: Input-Input Layer5 The Law will never be perfect , but its\n",
      "application should be just - this is what we are missing , in my\n",
      "opinion . <EOS> <pad> The Law will never be perfect , but its\n",
      "application should be just - this is what we are missing , in my\n",
      "opinion . <EOS> <pad> Input-Input Layer5 The Law will never be perfect\n",
      ", but its application sho...\n",
      "\n",
      "\n",
      "Doc ID: d4417c42-76be-46f1-8282-e9d2d92354df\n",
      "Text: Input-Input Layer5 The Law will never be perfect , but its\n",
      "application should be just - this is what we are missing , in my\n",
      "opinion . <EOS> <pad> The Law will never be perfect , but its\n",
      "application should be just - this is what we are missing , in my\n",
      "opinion . <EOS> <pad> Input-Input Layer5 The Law will never be perfect\n",
      ", but its application sho...\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Load PDF File\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "def load_documents_from_directory(input_files):\n",
    "    \"\"\"Load documents from the specified directory using SimpleDirectoryReader.\"\"\"\n",
    "    try:\n",
    "        reader = SimpleDirectoryReader(input_files=input_files)\n",
    "        documents = reader.load_data()\n",
    "        print(f\"Successfully loaded {len(documents)} document(s).\")\n",
    "        return documents\n",
    "    except FileNotFoundError as e:\n",
    "        raise RuntimeError(f\"File not found: {e}\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"An error occurred while loading documents: {e}\")\n",
    "\n",
    "# Specify the path to the text files\n",
    "input_files = [\"../artifacts/attention-is-all-you-need.pdf\"]\n",
    "\n",
    "# Load documents\n",
    "try:\n",
    "    documents = load_documents_from_directory(input_files)    # It will be needed in subsequent cells\n",
    "    for document in documents:\n",
    "        print(\"\\n\")\n",
    "        print(document)\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a VectorStoreIndex and query documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The document is about a neural network architecture for machine translation. The architecture is based on attention mechanisms and does not use recurrence or convolutions. The document also discusses the performance of the architecture on several machine translation tasks and compares it to other state-of-the-art models.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "def initialize_index_and_create_query_engine(documents):\n",
    "    \"\"\"Initializes the VectorStoreIndex from given documents and embedding model.\"\"\"\n",
    "    try:\n",
    "        index = VectorStoreIndex.from_documents(\n",
    "            documents=documents,\n",
    "            embed_model=embedding_model\n",
    "        )\n",
    "        # Adding local storage of index  - if you don't want to store then comment below line of code\n",
    "        index.storage_context.persist()\n",
    "        ###############################\n",
    "        query_engine = index.as_query_engine(llm=llm_instance)       \n",
    "        return query_engine\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create index or query engine: {e}\")\n",
    "\n",
    "try:\n",
    "    query_engine = initialize_index_and_create_query_engine(documents)\n",
    "    # query = input(\"Ask me anything about the document: \")    \n",
    "    query = \"What is the document is all about?\"        \n",
    "    response = query_engine.query(query)\n",
    "    print(response)    \n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during index initialization or querying: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Stored Index\n",
    "Note: Below code will only run once you have executed index.storage_context.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The document is about a neural network architecture for machine translation. The architecture is based on attention mechanisms and does not use recurrence or convolutions. The document also discusses the performance of the architecture on several machine translation tasks and compares it to other state-of-the-art models.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "index = load_index_from_storage(storage_context=storage_context)\n",
    "query_engine = index.as_query_engine(llm=llm_instance)\n",
    "response = query_engine.query(\"What is the document is all about?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customize the index\n",
    "By changing into the global settings module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The document is about a neural network architecture for machine translation. The architecture is based on attention mechanisms and does not use recurrence or convolutions. The document also discusses the performance of the architecture on several machine translation tasks and compares it to other state-of-the-art models.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = llm_instance\n",
    "Settings.embed_model = embedding_model\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "Settings.num_output = 512\n",
    "Settings.context_window = 3900\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "            documents=documents,\n",
    "            embed_model=embedding_model\n",
    "        )\n",
    "query_engine = index.as_query_engine(llm=llm_instance)\n",
    "response = query_engine.query(\"What is the document is all about?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Locally running open-source LLMs\n",
    "Below code needs GPU or TPU based configuration to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading LLM from HuggingFace locally\n",
    "\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM ##########################################################################################################################\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "system_prompt = \"\"\"<|SYSTEM|># StableLM Tuned (Alpha version)\n",
    "- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.\n",
    "- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.\n",
    "- StableLM will refuse to participate in anything that could harm a human.\n",
    "\"\"\"\n",
    "\n",
    "# This will wrap the default prompts that are internal to llama-index\n",
    "query_wrapper_prompt = PromptTemplate(\"<|USER|>{query_str}<|ASSISTANT|>\")\n",
    "\n",
    "llm_locally = HuggingFaceLLM(\n",
    "    context_window=4096,\n",
    "    max_new_tokens=256,\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": False},\n",
    "    system_prompt=system_prompt,\n",
    "    query_wrapper_prompt=query_wrapper_prompt,\n",
    "    tokenizer_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "    model_name=\"StabilityAI/stablelm-tuned-alpha-3b\",\n",
    "    device_map=\"auto\",\n",
    "    stopping_ids=[50278, 50279, 50277, 1, 0],\n",
    "    tokenizer_kwargs={\"max_length\": 4096},\n",
    "    # uncomment this if using CUDA to reduce memory usage\n",
    "    # model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing and querying with locally running LLMs\n",
    "index = VectorStoreIndex.from_documents(\n",
    "            documents=documents,\n",
    "            embed_model=embedding_model\n",
    "        )\n",
    "query_engine = index.as_query_engine(llm=llm_locally)\n",
    "response = query_engine.query(\"What is the document is all about?\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
